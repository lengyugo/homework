{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'F:/NLP/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "      # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-76f48eddfaf1>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "      # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "      # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \\\n",
    "                        + beta_regul * tf.nn.l2_loss(weights)\n",
    "\n",
    "      # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 19.666914\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 13.5%\n",
      "Minibatch loss at step 500: 2.276793\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 1000: 1.708365\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 1500: 0.956839\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2000: 0.775458\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 2500: 0.828236\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.775532\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里的beta_regul选择的是1e-3，看一下是否有更好的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10,i) for i in np.arange(-4,-2,0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:regul}\n",
    "            _, l, predictions = session.run(\n",
    "                  [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.semilogx(regul_val,accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('logistic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1-hidden layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node]))\n",
    "    \n",
    "    #\n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node,num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    logits = tf.matmul(lay_train, weights_2) + biases_2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "         beta_regul * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) )       \n",
    "\n",
    "      # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(lay_valid, weights_2) + biases_2)\n",
    "    \n",
    "    lay_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 676.471802\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 37.7%\n",
      "Minibatch loss at step 500: 198.382141\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 1000: 114.970322\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 68.790100\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2000: 41.323761\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 2500: 25.156862\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3000: 15.484298\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.3%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5d7b869f5a3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeta_regul\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mregul\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             _, l, predictions = session.run(\n\u001b[1;32m---> 20\u001b[1;33m                   [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0maccuracy_val_neural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10,i) for i in np.arange(-4,-2,0.1)]\n",
    "accuracy_val_neural = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:regul}\n",
    "            _, l, predictions = session.run(\n",
    "                  [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val_neural.append(accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.semilogx(regul_val,accuracy_val_neural)\n",
    "plt.grid(True)\n",
    "plt.title('logistic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node]))\n",
    "    \n",
    "    #\n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node,num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    logits = tf.matmul(lay_train, weights_2) + biases_2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))        \n",
    "\n",
    "      # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(lay_valid, weights_2) + biases_2)\n",
    "    \n",
    "    lay_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 365.148041\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 29.5%\n",
      "Minibatch loss at step 5: 102.823334\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 63.8%\n",
      "Minibatch loss at step 10: 53.578259\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 15: 19.859257\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 20: 1.410985\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 25: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 35: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 45: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 55: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 65: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 75: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 85: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 95: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.6%\n",
      "Test accuracy: 82.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_branch = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = ((step % num_branch) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 5 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-96fa4b1b4cc3>:28: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node]))\n",
    "    \n",
    "    #\n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node,num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    drop1 = tf.nn.dropout(lay_train,0.5)\n",
    "    logits = tf.matmul(drop1, weights_2) + biases_2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))        \n",
    "\n",
    "      # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(lay_valid, weights_2) + biases_2)\n",
    "    \n",
    "    lay_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 464.023682\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 20.0%\n",
      "Minibatch loss at step 2: 1298.329224\n",
      "Minibatch accuracy: 41.4%\n",
      "Validation accuracy: 26.6%\n",
      "Minibatch loss at step 4: 642.012634\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 51.0%\n",
      "Minibatch loss at step 6: 394.419495\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 59.5%\n",
      "Minibatch loss at step 8: 33.013435\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 10: 46.336048\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 12: 47.496021\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 14: 17.548189\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 16: 12.404629\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 75.1%\n",
      "Minibatch loss at step 18: 23.812737\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 20: 11.893568\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 22: 12.624208\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 24: 23.667330\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 26: 5.317602\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 28: 15.556385\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 30: 24.528902\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 32: 11.648530\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 34: 14.925523\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 36: 35.606522\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 38: 10.945330\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 40: 1.058993\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 42: 5.457448\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 44: 0.648056\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 46: 2.548189\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 48: 23.551355\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 50: 5.881589\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 52: 5.315289\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 54: 17.685480\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 56: 5.835161\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 58: 2.123459\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 60: 5.535496\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 62: 2.586094\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 64: 1.759931\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 66: 5.187663\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 68: 1.926361\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 70: 2.666066\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 74: 1.484272\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 76: 2.375821\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 78: 0.092824\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 80: 0.460905\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 82: 7.740542\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 84: 0.390323\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 86: 2.686080\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 88: 4.508563\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 90: 3.528404\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 96: 12.074308\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 100: 0.111719\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.1%\n",
      "Test accuracy: 83.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_branch = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        #offset = step % num_branch\n",
    "        offset = ((step % num_branch) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node1 = 1024\n",
    "num_hidden_node2 = 256\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node1],stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node1]))\n",
    "    \n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node1,num_hidden_node2],stddev=np.sqrt(2.0 / num_hidden_node1)))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_hidden_node2]))\n",
    "    \n",
    "    weights_3 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node2,num_labels],stddev=np.sqrt(2.0 / num_hidden_node2)))\n",
    "    biases_3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    lay_train2 = tf.nn.relu(tf.matmul(lay_train1, weights_2) + biases_2)\n",
    "    logits = tf.matmul(lay_train2, weights_3) + biases_3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "            beta_regul * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3))\n",
    "\n",
    "      # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5,global_step,1000,0.65,staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    lay_valid2 = tf.nn.relu(tf.matmul(lay_valid1, weights_2) + biases_2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay_valid2, weights_3) + biases_3)\n",
    "    \n",
    "    lay_test1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    lay_test2 = tf.nn.relu(tf.matmul(lay_test1, weights_2) + biases_2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test2, weights_3) + biases_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.356025\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 37.2%\n",
      "Minibatch loss at step 500: 1.001440\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1000: 0.912210\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 1500: 0.602747\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2000: 0.531688\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 88.2%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-748ce5183288>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         _, l, predictions = session.run(\n\u001b[1;32m---> 18\u001b[1;33m               [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Minibatch loss at step %d: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 三层网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node1 = 1024\n",
    "num_hidden_node2 = 256\n",
    "num_hidden_node3 = 128\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node1],stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node1]))\n",
    "    \n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node1,num_hidden_node2],stddev=np.sqrt(2.0 / num_hidden_node1)))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_hidden_node2]))\n",
    "    \n",
    "    weights_3 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node2,num_hidden_node3],stddev=np.sqrt(2.0 / num_hidden_node2)))\n",
    "    biases_3 = tf.Variable(tf.zeros([num_hidden_node3]))\n",
    "    weights_4 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node3,num_labels],stddev=np.sqrt(2.0 / num_hidden_node3)))\n",
    "    biases_4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    lay_train2 = tf.nn.relu(tf.matmul(lay_train1, weights_2) + biases_2)\n",
    "    lay_train3 = tf.nn.relu(tf.matmul(lay_train2, weights_3) + biases_3)\n",
    "    logits = tf.matmul(lay_train3, weights_4) + biases_4\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "            beta_regul * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_4))\n",
    "\n",
    "      # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5,global_step,4000,0.65,staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    lay_valid2 = tf.nn.relu(tf.matmul(lay_valid1, weights_2) + biases_2)\n",
    "    lay_valid3 = tf.nn.relu(tf.matmul(lay_valid2, weights_3) + biases_3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay_valid3, weights_4) + biases_4)\n",
    "    \n",
    "    lay_test1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    lay_test2 = tf.nn.relu(tf.matmul(lay_test1, weights_2) + biases_2)\n",
    "    lay_test3 = tf.nn.relu(tf.matmul(lay_test2, weights_3) + biases_3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test3, weights_4) + biases_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 9001 ##可以增大\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
