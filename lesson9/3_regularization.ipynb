{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'E:/NLP/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "      # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-76f48eddfaf1>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "      # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "      # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \\\n",
    "                        + beta_regul * tf.nn.l2_loss(weights)\n",
    "\n",
    "      # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 24.374178\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 5.7%\n",
      "Minibatch loss at step 500: 2.340827\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 75.9%\n",
      "Minibatch loss at step 1000: 1.723256\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1500: 1.033627\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 2000: 0.814605\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 2500: 0.826681\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.779557\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里的beta_regul选择的是1e-3，看一下是否有更好的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10,i) for i in np.arange(-4,-2,0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:regul}\n",
    "            _, l, predictions = session.run(\n",
    "                  [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val.append(accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.semilogx(regul_val,accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('logistic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1-hidden layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node]))\n",
    "    \n",
    "    #\n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node,num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    logits = tf.matmul(lay_train, weights_2) + biases_2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "         beta_regul * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) )       \n",
    "\n",
    "      # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(lay_valid, weights_2) + biases_2)\n",
    "    \n",
    "    lay_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 664.010803\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 40.7%\n",
      "Minibatch loss at step 500: 200.222900\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 1000: 114.950859\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1500: 68.663292\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2000: 41.180202\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 2500: 25.150845\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3000: 15.498733\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10,i) for i in np.arange(-4,-2,0.1)]\n",
    "accuracy_val_neural = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:regul}\n",
    "            _, l, predictions = session.run(\n",
    "                  [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_val_neural.append(accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.semilogx(regul_val,accuracy_val_neural)\n",
    "plt.grid(True)\n",
    "plt.title('logistic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node]))\n",
    "    \n",
    "    #\n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node,num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    logits = tf.matmul(lay_train, weights_2) + biases_2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))        \n",
    "\n",
    "      # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(lay_valid, weights_2) + biases_2)\n",
    "    \n",
    "    lay_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 356.971252\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 37.7%\n",
      "Minibatch loss at step 5: 184.757721\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 61.1%\n",
      "Minibatch loss at step 10: 56.277603\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 15: 68.043655\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 20: 0.697439\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 25: 0.493527\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 72.1%\n",
      "Minibatch loss at step 30: 4.253363\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 35: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 45: 0.000029\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 55: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 60: 0.000013\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 65: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 75: 0.000008\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 85: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 90: 0.000006\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 95: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 74.0%\n",
      "Test accuracy: 81.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_branch = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = ((step % num_branch) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul:1e-3}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 5 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-96fa4b1b4cc3>:28: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node]))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node]))\n",
    "    \n",
    "    #\n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node,num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    drop1 = tf.nn.dropout(lay_train,0.5)\n",
    "    logits = tf.matmul(drop1, weights_2) + biases_2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))        \n",
    "\n",
    "      # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(lay_valid, weights_2) + biases_2)\n",
    "    \n",
    "    lay_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 516.923889\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 25.5%\n",
      "Minibatch loss at step 2: 1142.102783\n",
      "Minibatch accuracy: 43.0%\n",
      "Validation accuracy: 42.4%\n",
      "Minibatch loss at step 4: 348.107635\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 60.0%\n",
      "Minibatch loss at step 6: 207.334503\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 66.8%\n",
      "Minibatch loss at step 8: 56.561127\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 10: 80.521698\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 70.7%\n",
      "Minibatch loss at step 12: 74.241295\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 14: 56.340584\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 16: 12.882603\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 18: 32.602188\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 73.6%\n",
      "Minibatch loss at step 20: 4.344163\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 22: 13.075582\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 24: 27.731037\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 26: 5.258194\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 28: 12.483070\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 30: 27.916096\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 32: 3.921762\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 34: 10.641744\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 36: 13.758510\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 38: 1.644623\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 40: 2.862909\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 42: 18.396242\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 44: 1.724075\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 46: 0.000020\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 48: 11.967214\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 50: 0.865887\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 52: 2.114613\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 54: 6.261532\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 56: 0.907964\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 58: 1.438755\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 60: 2.252364\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 62: 0.178299\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 64: 3.176548\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 66: 2.121136\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 68: 0.678258\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 70: 0.892008\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 72: 9.943697\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 76: 0.242013\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 78: 0.960548\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 80: 0.448946\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 82: 2.216411\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 84: 0.981868\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 86: 0.043187\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 90: 2.367359\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 94: 1.723994\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 96: 0.343345\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 100: 0.547998\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 77.2%\n",
      "Test accuracy: 84.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_branch = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        #offset = step % num_branch\n",
    "        offset = ((step % num_branch) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node1 = 1024\n",
    "num_hidden_node2 = 256\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node1],stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node1]))\n",
    "    \n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node1,num_hidden_node2],stddev=np.sqrt(2.0 / num_hidden_node1)))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_hidden_node2]))\n",
    "    \n",
    "    weights_3 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node2,num_labels],stddev=np.sqrt(2.0 / num_hidden_node2)))\n",
    "    biases_3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    lay_train2 = tf.nn.relu(tf.matmul(lay_train1, weights_2) + biases_2)\n",
    "    logits = tf.matmul(lay_train2, weights_3) + biases_3\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "            beta_regul * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3))\n",
    "\n",
    "      # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5,global_step,1000,0.65,staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    lay_valid2 = tf.nn.relu(tf.matmul(lay_valid1, weights_2) + biases_2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay_valid2, weights_3) + biases_3)\n",
    "    \n",
    "    lay_test1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    lay_test2 = tf.nn.relu(tf.matmul(lay_test1, weights_2) + biases_2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test2, weights_3) + biases_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.387552\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 34.1%\n",
      "Minibatch loss at step 500: 0.992636\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1000: 0.937559\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 1500: 0.588084\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2000: 0.544264\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 2500: 0.549017\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3000: 0.558289\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 3500: 0.579430\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 4000: 0.461265\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4500: 0.449420\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 5000: 0.505468\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 5500: 0.496626\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 6000: 0.579937\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6500: 0.402469\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7000: 0.518003\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 7500: 0.505601\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 8000: 0.582524\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8500: 0.430204\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 9000: 0.482782\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.2%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 三层网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_node1 = 1024\n",
    "num_hidden_node2 = 256\n",
    "num_hidden_node3 = 128\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                        shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "      # Variables.\n",
    "    weights_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_hidden_node1],stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases_1 = tf.Variable(tf.zeros([num_hidden_node1]))\n",
    "    \n",
    "    weights_2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node1,num_hidden_node2],stddev=np.sqrt(2.0 / num_hidden_node1)))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_hidden_node2]))\n",
    "    \n",
    "    weights_3 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node2,num_hidden_node3],stddev=np.sqrt(2.0 / num_hidden_node2)))\n",
    "    biases_3 = tf.Variable(tf.zeros([num_hidden_node3]))\n",
    "    weights_4 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_node3,num_labels],stddev=np.sqrt(2.0 / num_hidden_node3)))\n",
    "    biases_4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "\n",
    "      # Training computation.\n",
    "    lay_train1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    lay_train2 = tf.nn.relu(tf.matmul(lay_train1, weights_2) + biases_2)\n",
    "    lay_train3 = tf.nn.relu(tf.matmul(lay_train2, weights_3) + biases_3)\n",
    "    logits = tf.matmul(lay_train3, weights_4) + biases_4\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "            beta_regul * (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(weights_4))\n",
    "\n",
    "      # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5,global_step,4000,0.65,staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    lay_valid1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1) + biases_1)\n",
    "    lay_valid2 = tf.nn.relu(tf.matmul(lay_valid1, weights_2) + biases_2)\n",
    "    lay_valid3 = tf.nn.relu(tf.matmul(lay_valid2, weights_3) + biases_3)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay_valid3, weights_4) + biases_4)\n",
    "    \n",
    "    lay_test1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_1) + biases_1)\n",
    "    lay_test2 = tf.nn.relu(tf.matmul(lay_test1, weights_2) + biases_2)\n",
    "    lay_test3 = tf.nn.relu(tf.matmul(lay_test2, weights_3) + biases_3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay_test3, weights_4) + biases_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.421968\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 35.2%\n",
      "Minibatch loss at step 1000: 0.955262\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2000: 0.526072\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 3000: 0.565207\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 4000: 0.450828\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 5000: 0.487830\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 6000: 0.611564\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 7000: 0.539068\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 8000: 0.615691\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 9000: 0.442522\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.5%\n",
      "Test accuracy: 95.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001 ##可以增大\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                            valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
