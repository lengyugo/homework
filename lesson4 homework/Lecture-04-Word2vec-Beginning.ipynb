{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([11231, 999, 123142])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([-10, 10, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vec):\n",
    "    vec -= np.max(vec)\n",
    "    exp = np.exp(vec)\n",
    "    return exp / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.71390701e-15, 8.31528028e-07, 9.99999168e-01])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-04 基于维基百科的词向量构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章，你将使用Gensim和维基百科获得你的第一批词向量，并且感受词向量的基本过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.kaggleusercontent.com/kf/1018109/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JNNggcCCDcYEypvp7ZDwOA.cM9CuDpuCKo0K_ZkMFLAUvhfip0P6SRZ4LddwgTtgwz8pQy1dZeGVJWi6u81KSpAFNSi7YximVVJbPw8xsFySdWlqoUwvSER-LLIRfmlpsCvtDt90NaLYT2FHlwl0tfF-1MKtiFsWlGQ8LGo40hL3ccBSwMZy214kGJf9bNkW_g.kZbF5sgN5qha3zhjilfSDg/__results___files/__results___9_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-01: Download Wikipedia Chinese Corpus: https://dumps.wikimedia.org/zhwiki/20190720/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：使用维基百科下载中文语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-02: Using https://github.com/attardi/wikiextractor to extract the wikipedia corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：使用python wikipedia extractor抽取维基百科的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##WikiCorpus从wiki中提取\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "from zhconv import convert\n",
    "import re,jieba\n",
    "\n",
    "re.compile(\"[\\u4e00-\\u9fa5]\")\n",
    "def reduce_data():\n",
    "    filename = 'F:/NLP/wikiextractor/zhwiki-20190720-pages-articles-multistream.xml.bz2'\n",
    "    f = open('extrect_clean_Data.txt','w',encoding='utf-8')\n",
    "    wiki =WikiCorpus(filename,lemmatize=False,dictionary={})\n",
    "    i = 0\n",
    "    for text in wiki.get_texts():\n",
    "        word_list = []\n",
    "        for lines in text:\n",
    "            line = convert(lines,'zh-hans') ##繁体转化成简体 ，对于大文本不太适用\n",
    "            line = ''.join(re.compile(\"[\\u4e00-\\u9fa5]\").findall(line)) ##去除英文词\n",
    "            line = jieba.cut(line)\n",
    "            \n",
    "            for word in line:\n",
    "                word_list.append(word)\n",
    "        f.write(''.join(word_list) + '\\n')\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-03: Using gensim get word vectors: \n",
    "\n",
    "Reference: \n",
    "\n",
    "+ https://radimrehurek.com/gensim/models/word2vec.html\n",
    "+ https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三步：参考Gensim的文档和Kaggle的参考文档，获得词向量。 注意，你要使用Jieba分词把维基百科的内容切分成一个一个单词，然后存进新的文件中。然后，你需要用Gensim的LineSentence这个类进行文件的读取。\n",
    "\n",
    "在训练成词向量Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-01 11:45:49,543 : INFO : collecting all words and their counts\n",
      "2019-08-01 11:45:49,579 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-08-01 11:45:55,804 : INFO : PROGRESS: at sentence #10000, processed 12501841 words, keeping 596928 word types\n",
      "2019-08-01 11:46:00,643 : INFO : PROGRESS: at sentence #20000, processed 21660397 words, keeping 855890 word types\n",
      "2019-08-01 11:46:05,061 : INFO : PROGRESS: at sentence #30000, processed 29863149 words, keeping 1043934 word types\n",
      "2019-08-01 11:46:09,075 : INFO : PROGRESS: at sentence #40000, processed 37484137 words, keeping 1210401 word types\n",
      "2019-08-01 11:46:12,888 : INFO : PROGRESS: at sentence #50000, processed 44654216 words, keeping 1350616 word types\n",
      "2019-08-01 11:46:16,593 : INFO : PROGRESS: at sentence #60000, processed 51423269 words, keeping 1484206 word types\n",
      "2019-08-01 11:46:19,995 : INFO : PROGRESS: at sentence #70000, processed 57914182 words, keeping 1602361 word types\n",
      "2019-08-01 11:46:23,249 : INFO : PROGRESS: at sentence #80000, processed 64024699 words, keeping 1717069 word types\n",
      "2019-08-01 11:46:26,414 : INFO : PROGRESS: at sentence #90000, processed 70046837 words, keeping 1819809 word types\n",
      "2019-08-01 11:46:29,611 : INFO : PROGRESS: at sentence #100000, processed 76113010 words, keeping 1919043 word types\n",
      "2019-08-01 11:46:32,797 : INFO : PROGRESS: at sentence #110000, processed 82029747 words, keeping 2009484 word types\n",
      "2019-08-01 11:46:35,672 : INFO : PROGRESS: at sentence #120000, processed 87405270 words, keeping 2092722 word types\n",
      "2019-08-01 11:46:38,764 : INFO : PROGRESS: at sentence #130000, processed 93228594 words, keeping 2182938 word types\n",
      "2019-08-01 11:46:41,789 : INFO : PROGRESS: at sentence #140000, processed 98818079 words, keeping 2267953 word types\n",
      "2019-08-01 11:46:44,812 : INFO : PROGRESS: at sentence #150000, processed 104441798 words, keeping 2348653 word types\n",
      "2019-08-01 11:46:47,775 : INFO : PROGRESS: at sentence #160000, processed 109993049 words, keeping 2429824 word types\n",
      "2019-08-01 11:46:50,818 : INFO : PROGRESS: at sentence #170000, processed 115827656 words, keeping 2505016 word types\n",
      "2019-08-01 11:46:53,649 : INFO : PROGRESS: at sentence #180000, processed 121074837 words, keeping 2576845 word types\n",
      "2019-08-01 11:46:56,273 : INFO : PROGRESS: at sentence #190000, processed 125933468 words, keeping 2647737 word types\n",
      "2019-08-01 11:46:58,924 : INFO : PROGRESS: at sentence #200000, processed 130812415 words, keeping 2718880 word types\n",
      "2019-08-01 11:47:01,594 : INFO : PROGRESS: at sentence #210000, processed 135772518 words, keeping 2781604 word types\n",
      "2019-08-01 11:47:04,522 : INFO : PROGRESS: at sentence #220000, processed 141065833 words, keeping 2838575 word types\n",
      "2019-08-01 11:47:07,377 : INFO : PROGRESS: at sentence #230000, processed 146418900 words, keeping 2904486 word types\n",
      "2019-08-01 11:47:10,122 : INFO : PROGRESS: at sentence #240000, processed 151587065 words, keeping 2959610 word types\n",
      "2019-08-01 11:47:12,999 : INFO : PROGRESS: at sentence #250000, processed 156978147 words, keeping 3020533 word types\n",
      "2019-08-01 11:47:15,960 : INFO : PROGRESS: at sentence #260000, processed 162563378 words, keeping 3085764 word types\n",
      "2019-08-01 11:47:18,907 : INFO : PROGRESS: at sentence #270000, processed 168053249 words, keeping 3143148 word types\n",
      "2019-08-01 11:47:21,741 : INFO : PROGRESS: at sentence #280000, processed 173366393 words, keeping 3202164 word types\n",
      "2019-08-01 11:47:24,337 : INFO : PROGRESS: at sentence #290000, processed 178088245 words, keeping 3253760 word types\n",
      "2019-08-01 11:47:27,061 : INFO : PROGRESS: at sentence #300000, processed 182954205 words, keeping 3304992 word types\n",
      "2019-08-01 11:47:29,695 : INFO : PROGRESS: at sentence #310000, processed 187641405 words, keeping 3358397 word types\n",
      "2019-08-01 11:47:32,590 : INFO : PROGRESS: at sentence #320000, processed 192968823 words, keeping 3416303 word types\n",
      "2019-08-01 11:47:35,317 : INFO : PROGRESS: at sentence #330000, processed 197924831 words, keeping 3475133 word types\n",
      "2019-08-01 11:47:37,863 : INFO : PROGRESS: at sentence #340000, processed 202547599 words, keeping 3524194 word types\n",
      "2019-08-01 11:47:38,783 : INFO : collected 3543637 word types from a corpus of 204242188 raw words and 343705 sentences\n",
      "2019-08-01 11:47:38,784 : INFO : Loading a fresh vocabulary\n",
      "2019-08-01 11:47:44,018 : INFO : effective_min_count=5 retains 837574 unique words (23% of original 3543637, drops 2706063)\n",
      "2019-08-01 11:47:44,020 : INFO : effective_min_count=5 leaves 200124009 word corpus (97% of original 204242188, drops 4118179)\n",
      "2019-08-01 11:47:48,732 : INFO : deleting the raw counts dictionary of 3543637 items\n",
      "2019-08-01 11:47:48,907 : INFO : sample=0.001 downsamples 15 most-common words\n",
      "2019-08-01 11:47:48,908 : INFO : downsampling leaves estimated 190558893 word corpus (95.2% of prior 200124009)\n",
      "2019-08-01 11:47:52,794 : INFO : estimated required memory for 837574 words and 300 dimensions: 2428964600 bytes\n",
      "2019-08-01 11:47:52,796 : INFO : resetting layer weights\n",
      "2019-08-01 11:48:16,335 : INFO : training model with 4 workers on 837574 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-08-01 11:48:17,425 : INFO : EPOCH 1 - PROGRESS: at 0.04% examples, 305963 words/s, in_qsize 8, out_qsize 1\n",
      "2019-08-01 11:48:18,432 : INFO : EPOCH 1 - PROGRESS: at 0.09% examples, 330160 words/s, in_qsize 6, out_qsize 0\n",
      "2019-08-01 11:48:19,477 : INFO : EPOCH 1 - PROGRESS: at 0.16% examples, 343448 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:20,492 : INFO : EPOCH 1 - PROGRESS: at 0.22% examples, 335255 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-01 11:48:21,516 : INFO : EPOCH 1 - PROGRESS: at 0.27% examples, 326808 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-01 11:48:22,520 : INFO : EPOCH 1 - PROGRESS: at 0.35% examples, 334902 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-01 11:48:23,532 : INFO : EPOCH 1 - PROGRESS: at 0.44% examples, 342289 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:24,558 : INFO : EPOCH 1 - PROGRESS: at 0.52% examples, 345761 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:25,602 : INFO : EPOCH 1 - PROGRESS: at 0.60% examples, 345476 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-01 11:48:26,661 : INFO : EPOCH 1 - PROGRESS: at 0.67% examples, 338606 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:27,685 : INFO : EPOCH 1 - PROGRESS: at 0.73% examples, 333830 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:28,736 : INFO : EPOCH 1 - PROGRESS: at 0.82% examples, 335900 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-01 11:48:29,740 : INFO : EPOCH 1 - PROGRESS: at 0.89% examples, 332833 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-01 11:48:30,752 : INFO : EPOCH 1 - PROGRESS: at 1.00% examples, 332573 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-01 11:48:31,792 : INFO : EPOCH 1 - PROGRESS: at 1.08% examples, 333195 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:32,795 : INFO : EPOCH 1 - PROGRESS: at 1.17% examples, 334358 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:33,817 : INFO : EPOCH 1 - PROGRESS: at 1.27% examples, 336480 words/s, in_qsize 6, out_qsize 0\n",
      "2019-08-01 11:48:34,836 : INFO : EPOCH 1 - PROGRESS: at 1.35% examples, 335288 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-01 11:48:35,837 : INFO : EPOCH 1 - PROGRESS: at 1.43% examples, 337279 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:36,877 : INFO : EPOCH 1 - PROGRESS: at 1.56% examples, 339403 words/s, in_qsize 6, out_qsize 1\n",
      "2019-08-01 11:48:37,887 : INFO : EPOCH 1 - PROGRESS: at 1.70% examples, 341196 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:38,925 : INFO : EPOCH 1 - PROGRESS: at 1.79% examples, 342486 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:39,948 : INFO : EPOCH 1 - PROGRESS: at 1.89% examples, 344142 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-01 11:48:40,950 : INFO : EPOCH 1 - PROGRESS: at 2.00% examples, 345251 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-01 11:48:41,972 : INFO : EPOCH 1 - PROGRESS: at 2.11% examples, 346731 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-01 11:48:42,995 : INFO : EPOCH 1 - PROGRESS: at 2.22% examples, 347825 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-01 11:48:44,024 : INFO : EPOCH 1 - PROGRESS: at 2.34% examples, 349215 words/s, in_qsize 7, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-01 11:48:45,034 : INFO : EPOCH 1 - PROGRESS: at 2.46% examples, 350348 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:46,043 : INFO : EPOCH 1 - PROGRESS: at 2.57% examples, 351143 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:47,051 : INFO : EPOCH 1 - PROGRESS: at 2.71% examples, 352271 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-01 11:48:48,067 : INFO : EPOCH 1 - PROGRESS: at 2.80% examples, 352857 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:49,071 : INFO : EPOCH 1 - PROGRESS: at 2.91% examples, 353823 words/s, in_qsize 6, out_qsize 0\n",
      "2019-08-01 11:48:50,090 : INFO : EPOCH 1 - PROGRESS: at 3.00% examples, 354259 words/s, in_qsize 6, out_qsize 1\n",
      "2019-08-01 11:48:51,098 : INFO : EPOCH 1 - PROGRESS: at 3.11% examples, 354851 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-01 11:48:52,107 : INFO : EPOCH 1 - PROGRESS: at 3.24% examples, 355348 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-01 11:48:53,136 : INFO : EPOCH 1 - PROGRESS: at 3.37% examples, 355859 words/s, in_qsize 7, out_qsize 1\n",
      "2019-08-01 11:48:54,177 : INFO : EPOCH 1 - PROGRESS: at 3.49% examples, 356209 words/s, in_qsize 8, out_qsize 0\n",
      "2019-08-01 11:48:55,183 : INFO : EPOCH 1 - PROGRESS: at 3.62% examples, 356497 words/s, in_qsize 7, out_qsize 0\n",
      "2019-08-01 11:48:56,210 : INFO : EPOCH 1 - PROGRESS: at 3.75% examples, 356712 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-f52a42b0990e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'extract_data.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLineSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wiki_word2vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[0;32m    764\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "train_data = open('extract_data.txt','r',encoding='utf-8')\n",
    "model = Word2Vec(LineSentence(train_data),size=300,window=5,min_count=5,workers=4)\n",
    "model.save('wiki_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##想要继续训练\n",
    "\n",
    "model = Word2Vec.load('zhwiki.word2vec')\n",
    "model.train(LineSentence(sentence),total_examples=model.corpus_count,epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-04: Using some words to test your preformance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四步，测试同义词，找几个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-01 11:49:03,583 : INFO : loading Word2Vec object from zhwiki.word2vec\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "2019-08-01 11:49:06,310 : INFO : loading wv recursively from zhwiki.word2vec.wv.* with mmap=None\n",
      "2019-08-01 11:49:06,313 : INFO : loading vectors from zhwiki.word2vec.wv.vectors.npy with mmap=None\n",
      "2019-08-01 11:49:15,080 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-08-01 11:49:15,081 : INFO : loading vocabulary recursively from zhwiki.word2vec.vocabulary.* with mmap=None\n",
      "2019-08-01 11:49:15,082 : INFO : loading trainables recursively from zhwiki.word2vec.trainables.* with mmap=None\n",
      "2019-08-01 11:49:15,083 : INFO : loading syn1neg from zhwiki.word2vec.trainables.syn1neg.npy with mmap=None\n",
      "2019-08-01 11:49:23,868 : INFO : setting ignored attribute cum_table to None\n",
      "2019-08-01 11:49:23,869 : INFO : loaded zhwiki.word2vec\n",
      "2019-08-01 11:49:26,579 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('马化腾', 0.7377442121505737),\n",
       " ('丁磊', 0.6828475594520569),\n",
       " ('李彦宏', 0.6595268845558167),\n",
       " ('俞敏洪', 0.6383185386657715),\n",
       " ('罗永浩', 0.6376563310623169),\n",
       " ('阿里巴巴', 0.6375777125358582),\n",
       " ('雷军', 0.6349774599075317),\n",
       " ('徐翔', 0.6332528591156006),\n",
       " ('王健林', 0.6309305429458618),\n",
       " ('任正非', 0.6264487504959106)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec.load('zhwiki.word2vec')\n",
    "model.wv.most_similar('马云')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('水果', 0.5278795957565308),\n",
       " ('青椒', 0.5267738103866577),\n",
       " ('西瓜', 0.5254215002059937),\n",
       " ('柠檬', 0.509736180305481),\n",
       " ('饼干', 0.5089078545570374),\n",
       " ('椰子', 0.5078338384628296),\n",
       " ('爆米花', 0.5049172043800354),\n",
       " ('辣椒', 0.5043478608131409),\n",
       " ('凤梨', 0.501450777053833),\n",
       " ('焦糖', 0.5005712509155273)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('香蕉')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.48289788e+00, -6.76019251e-01,  2.56539136e-01, -8.64550829e-01,\n",
       "        1.19929349e+00, -4.00301635e-01, -1.64934710e-01, -3.57855290e-01,\n",
       "        3.99911910e-01,  1.00767855e-02,  5.73509037e-01,  5.23178160e-01,\n",
       "        1.52857995e+00,  2.17044026e-01, -1.09162211e+00, -9.02919471e-01,\n",
       "        4.85750169e-01,  3.77947360e-01,  1.02736700e+00, -1.21818161e+00,\n",
       "       -7.54823983e-01,  1.24467924e-01, -7.27465451e-01,  1.60460353e+00,\n",
       "        2.09078264e+00,  6.87431246e-02,  6.21666729e-01,  4.01544780e-01,\n",
       "        1.25323665e+00,  6.25070453e-01,  1.28955317e+00,  1.18652487e+00,\n",
       "        2.09684610e+00, -8.83195639e-01,  1.01633728e+00, -5.36444299e-02,\n",
       "        1.47403538e-01, -6.11855567e-01, -4.36634588e+00, -2.78677285e-01,\n",
       "       -1.75487494e+00, -8.63753498e-01, -6.19995832e-01, -3.40382278e-01,\n",
       "       -5.30526936e-01, -1.96424246e-01,  1.63355112e+00, -8.04435968e-01,\n",
       "       -8.11594844e-01, -9.53248739e-01, -2.02779934e-01, -1.59947181e+00,\n",
       "       -2.20533833e-01,  3.12406659e-01,  1.77912474e-01,  1.87636828e+00,\n",
       "       -6.16338193e-01,  1.17730486e+00,  3.33707839e-01, -5.79659998e-01,\n",
       "       -8.50533128e-01, -3.17812681e-01,  1.54169858e+00, -4.85060215e-01,\n",
       "       -2.01609421e+00,  5.79630435e-01, -1.39217544e+00,  2.48759270e-01,\n",
       "       -5.92478625e-02, -1.57743621e+00,  1.53782284e+00, -2.41164155e-02,\n",
       "       -2.45022917e+00, -2.87605095e+00,  1.60016596e+00,  1.50435204e-02,\n",
       "        1.30838239e+00,  7.75311768e-01, -1.78350732e-01,  1.68957198e+00,\n",
       "       -3.82016629e-01,  2.79557467e+00, -1.15486705e+00, -1.65574729e+00,\n",
       "        9.41947162e-01, -2.79978561e+00, -7.55764961e-01, -5.64932346e-01,\n",
       "        2.37708735e+00, -8.73033464e-01, -2.11546302e+00, -1.16579719e-02,\n",
       "       -3.68532777e-01,  8.02101791e-01,  8.06687593e-01, -1.09446704e-01,\n",
       "        1.41748190e+00, -2.01613486e-01,  6.71816051e-01, -1.79793447e-01,\n",
       "       -8.25396359e-01, -2.00763202e+00, -9.04269814e-01, -1.62764385e-01,\n",
       "        2.36726299e-01,  9.05536056e-01,  1.25886679e-01,  1.38186738e-01,\n",
       "       -4.30442905e-03, -4.39465702e-01,  3.52541693e-02,  3.38601899e+00,\n",
       "       -1.44605875e+00, -8.03558588e-01, -2.00380951e-01,  1.14539742e+00,\n",
       "       -1.63996971e+00,  2.70152479e-01,  1.73210335e+00,  6.14764810e-01,\n",
       "        3.02361876e-01,  2.86640316e-01,  7.49417305e-01, -1.74265027e+00,\n",
       "       -8.34700167e-01, -1.28510371e-01, -1.51148534e+00,  1.28486753e+00,\n",
       "        5.94605088e-01,  1.50708675e+00,  8.21737468e-01, -2.83752322e+00,\n",
       "       -6.56899996e-03, -4.22453910e-01, -1.13152838e+00,  1.89731741e+00,\n",
       "        6.32450998e-01, -7.94860661e-01, -3.63937169e-01, -2.02338195e+00,\n",
       "       -1.87658876e-01, -1.12198400e+00,  3.95872220e-02,  2.15643859e+00,\n",
       "       -6.25053465e-01, -1.46189988e+00,  2.10737062e+00,  1.74197352e+00,\n",
       "       -1.50974321e+00, -3.38422823e+00,  1.43256426e+00,  1.72820735e+00,\n",
       "        1.82405984e+00,  4.49955650e-02,  5.49538791e-01,  2.17934108e+00,\n",
       "        1.20022643e+00,  2.13553846e-01,  9.39118803e-01,  2.72875667e+00,\n",
       "       -2.52785981e-01, -1.67221284e+00, -4.04219419e-01, -1.25029314e+00,\n",
       "        2.39891425e-01,  1.07896578e+00,  5.09357035e-01, -7.53271356e-02,\n",
       "       -2.37474535e-02,  6.65647835e-02,  1.45007825e+00, -4.36573684e-01,\n",
       "        1.28641582e+00, -5.48078120e-01,  9.10656631e-01,  9.12722766e-01,\n",
       "        5.34416318e-01, -1.23011398e+00,  1.87416136e+00,  1.68832272e-01,\n",
       "        4.50876653e-01,  1.91036224e+00,  5.97408181e-03, -9.24231410e-01,\n",
       "        1.24156702e+00,  7.80620635e-01, -2.21463561e+00,  2.08853436e+00,\n",
       "        1.49813581e+00, -1.07553184e-01,  1.41646469e+00,  8.71220767e-01,\n",
       "       -5.35665035e-01, -1.71566918e-01, -1.55060959e+00, -2.04754770e-01,\n",
       "        1.72959888e+00,  7.93947518e-01,  1.25035822e+00, -1.81754279e+00,\n",
       "       -2.72255570e-01, -1.50951314e+00, -7.34272063e-01, -5.22981286e-01,\n",
       "        2.52525568e+00, -6.67870045e-01, -2.14717239e-01,  5.74383497e-01,\n",
       "        3.47886533e-01, -1.25743997e+00, -7.06873894e-01, -7.05252945e-01,\n",
       "       -4.61652726e-02,  1.03556132e+00, -5.15399575e-01,  1.57589662e+00,\n",
       "        1.57066596e+00,  6.48106635e-01,  9.95371521e-01, -2.85997659e-01,\n",
       "       -1.36397779e+00,  1.51500857e+00,  1.02323103e+00, -4.06837575e-02,\n",
       "       -7.44072735e-01,  2.66439825e-01,  1.22972155e+00, -1.03885479e-01,\n",
       "        6.28167212e-01,  3.67147595e-01,  6.59333885e-01,  8.80124509e-01,\n",
       "        1.08520758e+00, -2.45412201e-01, -5.81032753e-01, -1.15907383e+00,\n",
       "        1.37033118e-02, -7.02913046e-01, -1.14387417e+00,  2.86647344e+00,\n",
       "       -4.99973893e-01,  7.70642340e-01, -3.30016762e-01,  2.47450501e-01,\n",
       "       -7.00627446e-01, -7.28114367e-01,  2.66201925e+00,  4.43490088e-01,\n",
       "       -4.09914136e-01,  2.61133170e+00,  1.01703596e+00, -3.94707412e-01,\n",
       "       -6.33054435e-01, -4.29697484e-01, -1.01461425e-01, -4.46455479e-01,\n",
       "       -2.21849823e+00, -5.55356562e-01, -9.18541789e-01,  3.02091509e-01,\n",
       "        9.18176651e-01,  1.83347332e+00, -1.83112335e+00,  9.56465721e-01,\n",
       "       -1.53134644e+00,  1.20280612e+00,  1.87105358e-01, -6.62623942e-01,\n",
       "       -1.05941415e+00,  1.39464545e+00, -3.14877093e-01,  1.88142622e+00,\n",
       "        3.32553178e-01,  1.91243541e+00,  9.28747714e-01, -6.80406272e-01,\n",
       "        5.23004591e-01, -9.11525130e-01, -6.97918117e-01,  2.55409241e-01,\n",
       "       -2.55841017e+00,  3.55017453e-01,  2.29208037e-01,  6.32480621e-01,\n",
       "        7.65931845e-01,  9.32058990e-02, -1.63426518e+00, -8.49197328e-01,\n",
       "        8.46059397e-02,  1.60479712e+00,  9.57475901e-01,  9.66865271e-02,\n",
       "       -2.69358188e-01,  1.21225953e+00,  8.79878938e-01,  2.36563492e+00,\n",
       "        2.09473157e+00,  1.13867986e+00, -3.49703074e-01,  1.74217737e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['北京']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47728378"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('大学','高中')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-05: Using visualization tools: https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第五步：使用Kaggle给出的T-SEN进行词向量的可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
